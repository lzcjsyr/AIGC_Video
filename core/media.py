"""
Media-related logic: opening image, images per segment, and TTS synthesis.
"""

from typing import Optional, Dict, Any, List, Tuple
import os
import concurrent.futures
import threading
import base64

from config import config
from prompts import OPENING_IMAGE_STYLES, COVER_IMAGE_STYLE_PRESETS, COVER_IMAGE_PROMPT_TEMPLATE
from core.utils import logger, ensure_directory_exists
from core.services import text_to_image_doubao, text_to_audio_bytedance, text_to_image_siliconflow
from prompts import IMAGE_STYLE_PRESETS, IMAGE_DESCRIPTION_PROMPT_TEMPLATE

import requests


def _download_to_path(url: str, output_path: str, error_msg: str = "ä¸‹è½½å¤±è´¥") -> None:
    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
        with open(output_path, 'wb') as f:
            f.write(resp.content)
    except Exception as e:
        raise ValueError(f"{error_msg}: {e}")


def _persist_image_result(image_result: Dict[str, str], output_path: str, error_msg: str) -> None:
    """æ ¹æ®è¿”å›ç±»å‹å†™å…¥å›¾åƒæ–‡ä»¶ï¼Œæ”¯æŒURLæˆ–base64"""
    ensure_directory_exists(os.path.dirname(output_path))

    if not image_result:
        raise ValueError(f"{error_msg}: ç©ºå“åº”")

    data_type = image_result.get("type")
    data_value = image_result.get("data")

    if not data_type or not data_value:
        raise ValueError(f"{error_msg}: å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ")

    try:
        if data_type == "url":
            _download_to_path(data_value, output_path, error_msg=error_msg)
        elif data_type == "b64":
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(data_value))
        else:
            raise ValueError(f"æœªçŸ¥çš„å›¾åƒæ•°æ®ç±»å‹: {data_type}")
    except Exception as e:
        raise ValueError(f"{error_msg}: {e}")


def generate_opening_image(image_server: str, model: str, opening_style: str,
                           image_size: str, output_dir: str, opening_quote: bool = True) -> Optional[str]:
    """ç”Ÿæˆå¼€åœºå›¾åƒï¼Œå…¼å®¹å¤šç§æœåŠ¡å•†"""
    if not opening_quote:
        return None
    try:
        prompt = OPENING_IMAGE_STYLES.get(opening_style)
        if not prompt:
            default_style = next(iter(OPENING_IMAGE_STYLES))
            logger.warning(f"æœªæ‰¾åˆ°å¼€åœºå›¾åƒé£æ ¼: {opening_style}ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼: {default_style}")
            prompt = OPENING_IMAGE_STYLES[default_style]
        prompt = str(prompt).strip()

        image_path = os.path.join(output_dir, "opening.png")

        if image_server == "siliconflow":
            image_result = text_to_image_siliconflow(
                prompt=prompt,
                size=image_size,
                model=model
            )
            _persist_image_result(image_result, image_path, "å¼€åœºå›¾åƒä¿å­˜å¤±è´¥")
        else:
            image_url = text_to_image_doubao(
                prompt=prompt,
                size=image_size,
                model=model
            )

            if not image_url:
                raise ValueError("å¼€åœºå›¾åƒç”Ÿæˆå¤±è´¥")

            _persist_image_result({"type": "url", "data": image_url}, image_path, "å¼€åœºå›¾åƒä¸‹è½½å¤±è´¥")

        logger.info(f"å¼€åœºå›¾åƒå·²ä¿å­˜: {image_path} (é£æ ¼: {opening_style})")
        print(f"å¼€åœºå›¾åƒå·²ä¿å­˜: {image_path}")
        return image_path
    except Exception as e:
        logger.warning(f"å¼€åœºå›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        return None


def _ensure_cover_style(style_id: str) -> Tuple[str, str]:
    """è·å–å°é¢é£æ ¼æè¿°ï¼Œè¿”å›(é£æ ¼id, style_text)ã€‚"""
    if style_id in COVER_IMAGE_STYLE_PRESETS:
        return style_id, COVER_IMAGE_STYLE_PRESETS[style_id]
    default_key = next(iter(COVER_IMAGE_STYLE_PRESETS))
    return default_key, COVER_IMAGE_STYLE_PRESETS[default_key]


def generate_cover_images(
    project_output_dir: str,
    image_server: str,
    model: str,
    image_size: str,
    style_id: str,
    count: int,
    video_title: str,
    content_title: str,
    cover_subtitle: str,
) -> Dict[str, Any]:
    """ç”Ÿæˆå°é¢å›¾åƒï¼Œä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œæ–‡ä»¶å cover_XX.pngã€‚"""
    try:
        if count < 1:
            count = 1

        os.makedirs(project_output_dir, exist_ok=True)
        style_key, style_text = _ensure_cover_style(style_id)
        prompt = COVER_IMAGE_PROMPT_TEMPLATE.format(
            video_title=video_title,
            content_title=content_title,
            cover_subtitle=cover_subtitle or video_title,
            style_block=style_text,
        )

        generated_paths: List[str] = []
        failures: List[str] = []

        for idx in range(1, count + 1):
            result = _generate_single_cover(
                image_server,
                model,
                image_size,
                prompt,
                project_output_dir,
                idx,
            )
            if result.get("success"):
                generated_paths.append(result["image_path"])
            else:
                failures.append(result.get("error", f"å°é¢{idx}ç”Ÿæˆå¤±è´¥"))

        return {
            "success": len(generated_paths) > 0,
            "cover_paths": generated_paths,
            "failures": failures,
            "style_id": style_key,
        }
    except Exception as e:
        raise ValueError(f"å°é¢å›¾åƒç”Ÿæˆé”™è¯¯: {e}")


def _generate_single_cover(
    image_server: str,
    model: str,
    image_size: str,
    prompt: str,
    project_output_dir: str,
    index: int,
) -> Dict[str, Any]:
    filename = f"cover_{index:02d}.png"
    image_path = os.path.join(project_output_dir, filename)

    try:
        if image_server == "siliconflow":
            image_result = text_to_image_siliconflow(
                prompt=prompt,
                size=image_size,
                model=model,
            )
            _persist_image_result(image_result, image_path, f"ä¿å­˜å°é¢å›¾åƒ {filename} å¤±è´¥")
        else:
            image_url = text_to_image_doubao(
                prompt=prompt,
                size=image_size,
                model=model,
            )
            if not image_url:
                raise ValueError("å°é¢å›¾åƒç”Ÿæˆè¿”å›ç©ºURL")
            _persist_image_result({"type": "url", "data": image_url}, image_path, f"ä¸‹è½½å°é¢å›¾åƒ {filename} å¤±è´¥")

        logger.info(f"å°é¢å›¾åƒå·²ä¿å­˜: {image_path}")
        print(f"å°é¢å›¾åƒå·²ä¿å­˜: {image_path}")
        return {"success": True, "image_path": image_path}
    except Exception as e:
        logger.warning(f"å°é¢å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        print(f"å°é¢å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def _generate_single_image(args) -> Dict[str, Any]:
    """ç”Ÿæˆå•ä¸ªå›¾åƒçš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰"""
    segment_index, final_prompt, model, image_size, output_dir, image_server = args

    print(f"æ­£åœ¨ç”Ÿæˆç¬¬{segment_index}æ®µå›¾åƒ...")
    logger.debug(f"ç¬¬{segment_index}æ®µå›¾åƒæç¤ºè¯: {final_prompt}")

    for attempt in range(3):
        try:
            image_path = os.path.join(output_dir, f"segment_{segment_index}.png")

            if image_server == "siliconflow":
                image_result = text_to_image_siliconflow(
                    prompt=final_prompt,
                    size=image_size,
                    model=model
                )
                _persist_image_result(image_result, image_path, f"ä¿å­˜ç¬¬{segment_index}æ®µå›¾åƒå¤±è´¥")
            else:
                image_url = text_to_image_doubao(
                    prompt=final_prompt,
                    size=image_size,
                    model=model
                )
                if not image_url:
                    raise ValueError("å›¾åƒç”Ÿæˆè¿”å›ç©ºURL")
                _persist_image_result({"type": "url", "data": image_url}, image_path, f"ä¸‹è½½ç¬¬{segment_index}æ®µå›¾åƒå¤±è´¥")
            print(f"ç¬¬{segment_index}æ®µå›¾åƒå·²ä¿å­˜: {image_path}")
            logger.info(f"ç¬¬{segment_index}æ®µå›¾åƒç”ŸæˆæˆåŠŸ: {image_path}")
            return {"success": True, "segment_index": segment_index, "image_path": image_path}
        except Exception as e:
            error_msg = str(e)
            is_sensitive_error = (
                "OutputImageSensitiveContentDetected" in error_msg or
                "sensitive" in error_msg.lower() or
                "content" in error_msg.lower()
            )
            if attempt < 2:
                if is_sensitive_error:
                    logger.warning(f"ç¬¬{segment_index}æ®µå›¾åƒæ¶‰åŠæ•æ„Ÿå†…å®¹ï¼Œå‡†å¤‡é‡è¯•ï¼ˆç¬¬{attempt + 2}/3æ¬¡ï¼‰")
                else:
                    logger.warning(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼š{error_msg}ï¼Œå‡†å¤‡é‡è¯•ï¼ˆç¬¬{attempt + 2}/3æ¬¡ï¼‰")
                continue
            else:
                if is_sensitive_error:
                    logger.warning(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼ˆæ•æ„Ÿå†…å®¹ï¼‰ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯ï¼š{error_msg}")
                else:
                    logger.warning(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯ï¼š{error_msg}")
                print(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡")

    return {"success": False, "segment_index": segment_index, "image_path": ""}


def generate_images_for_segments(
    image_server: str,
    model: str,
    script_data: Dict[str, Any],
    image_style_preset: str,
    image_size: str,
    output_dir: str,
    images_method: str = "keywords",
    keywords_data: Optional[Dict[str, Any]] = None,
    description_data: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """ä¸ºæ¯ä¸ªæ®µè½ç”Ÿæˆå›¾åƒï¼ˆæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ï¼‰"""
    try:
        try:
            image_style = IMAGE_STYLE_PRESETS.get(
                image_style_preset,
                next(iter(IMAGE_STYLE_PRESETS.values()))
            )
        except Exception:
            image_style = ""
        logger.info(
            f"ä½¿ç”¨å›¾åƒæœåŠ¡: {image_server}ï¼Œé£æ ¼: {image_style_preset} -> {image_style}ï¼Œæ¨¡å¼: {images_method}"
        )

        images_method = images_method or getattr(config, "SUPPORTED_IMAGE_METHODS", ["keywords"])[0]
        segments = script_data.get("segments", [])
        if not segments:
            raise ValueError("è„šæœ¬æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾åƒ")

        prompt_payload: List[tuple[int, str]] = []

        if images_method == "description":
            summary_text = (description_data or {}).get("summary", "").strip()
            if not summary_text:
                raise ValueError("ç¼ºå°‘æè¿°æ¨¡å¼æ‰€éœ€çš„å°ç»“å†…å®¹")
            template = IMAGE_DESCRIPTION_PROMPT_TEMPLATE
            default_style = getattr(
                config,
                "DESCRIPTION_DEFAULT_STYLE_GUIDANCE",
                "ç”»é¢éœ€ä¿æŒä¿¡æ¯æ¸…æ™°ã€æ„å›¾ç¨³å®šã€è‰²å½©å’Œè°ã€‚"
            )
            for segment in segments:
                segment_index = int(segment.get("index") or len(prompt_payload) + 1)
                segment_content = segment.get("content", "")
                style_block = image_style or default_style
                final_prompt = template.format(
                    summary=summary_text,
                    segment=segment_content,
                    style_block=style_block
                )
                prompt_payload.append((segment_index, final_prompt))
        else:
            if not keywords_data:
                raise ValueError("ç¼ºå°‘å…³é”®è¯æ•°æ®")
            keyword_segments = list(keywords_data.get("segments", []))
            if len(keyword_segments) < len(segments):
                keyword_segments.extend(
                    [{"keywords": [], "atmosphere": []}] * (len(segments) - len(keyword_segments))
                )
            for idx, segment in enumerate(segments, 1):
                segment_keywords = keyword_segments[idx - 1] if idx - 1 < len(keyword_segments) else {}
                keywords = segment_keywords.get("keywords", [])
                atmosphere = segment_keywords.get("atmosphere", [])
                style_part = f"[é£æ ¼] {image_style}" if image_style else ""
                content_parts: List[str] = []
                content_parts.extend(keywords)
                content_parts.extend(atmosphere)
                content_part = f"[å†…å®¹] {' | '.join(content_parts)}" if content_parts else ""
                sections = [part for part in [style_part, content_part] if part]
                final_prompt = "\n".join(sections) if sections else image_style
                if not final_prompt:
                    final_prompt = f"[å†…å®¹] {segment.get('content', '')}".strip()
                segment_index = segment.get('index') or idx
                prompt_payload.append((segment_index, final_prompt))

        if not prompt_payload:
            raise ValueError("æœªç”Ÿæˆæœ‰æ•ˆçš„æç¤ºè¯")

        max_workers = getattr(config, "MAX_CONCURRENT_IMAGE_GENERATION", 3)
        print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹ç”Ÿæˆå›¾åƒ...")

        segment_count = len(segments)
        image_paths: List[str] = [""] * segment_count
        failed_segments: List[int] = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_index = {
                executor.submit(
                    _generate_single_image,
                    (int(idx), prompt, model, image_size, output_dir, image_server)
                ): int(idx)
                for idx, prompt in prompt_payload
            }

            for future in concurrent.futures.as_completed(future_to_index):
                result = future.result()
                segment_index = int(result["segment_index"])
                position = segment_index - 1
                if result["success"] and 0 <= position < segment_count:
                    image_paths[position] = result["image_path"]
                else:
                    failed_segments.append(segment_index)
                    if 0 <= position < segment_count:
                        image_paths[position] = ""

        return {
            "image_paths": image_paths,
            "failed_segments": failed_segments,
        }

    except Exception as e:
        raise ValueError(f"å›¾åƒç”Ÿæˆé”™è¯¯: {e}")


def _synthesize_single_voice(args) -> Dict[str, Any]:
    """
    åˆæˆå•ä¸ªè¯­éŸ³çš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
    """
    segment_index, content, server, voice, output_dir = args
    
    print(f"æ­£åœ¨ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³...")
    
    audio_filename = f"voice_{segment_index}.wav"
    audio_path = os.path.join(output_dir, audio_filename)
    
    try:
        if server == "bytedance":
            success = text_to_audio_bytedance(
                text=content,
                output_filename=audio_path,
                voice=voice
            )
        else:
            return {"success": False, "segment_index": segment_index, "error": f"ä¸æ”¯æŒçš„TTSæœåŠ¡å•†: {server}"}
        
        if success:
            print(f"ç¬¬{segment_index}æ®µè¯­éŸ³å·²ä¿å­˜: {audio_path}")
            return {"success": True, "segment_index": segment_index, "audio_path": audio_path}
        else:
            return {"success": False, "segment_index": segment_index, "error": f"ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³å¤±è´¥"}
    
    except Exception as e:
        return {"success": False, "segment_index": segment_index, "error": str(e)}


def synthesize_voice_for_segments(server: str, voice: str, script_data: Dict[str, Any], output_dir: str) -> List[str]:
    """
    ä¸ºæ¯ä¸ªæ®µè½åˆæˆè¯­éŸ³ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ï¼‰
    """
    try:
        # å‡†å¤‡å¹¶å‘ä»»åŠ¡å‚æ•°
        task_args = []
        for segment in script_data["segments"]:
            segment_index = segment["index"]
            content = segment["content"]
            task_args.append((segment_index, content, server, voice, output_dir))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆæˆè¯­éŸ³
        max_workers = getattr(config, "MAX_CONCURRENT_VOICE_SYNTHESIS", 2)
        print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹åˆæˆè¯­éŸ³...")
        
        audio_paths: List[str] = [""] * len(task_args)  # é¢„åˆ†é…ä½ç½®
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {executor.submit(_synthesize_single_voice, args): args[0] for args in task_args}
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(future_to_index):
                result = future.result()
                segment_index = result["segment_index"]
                if result["success"]:
                    audio_paths[segment_index - 1] = result["audio_path"]
                else:
                    error_msg = result.get("error", f"ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³å¤±è´¥")
                    raise ValueError(error_msg)

        # è¯­éŸ³åˆæˆå®Œæˆåï¼Œç«‹å³å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶
        print("ğŸ¬ å¼€å§‹å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶...")
        try:
            srt_path = export_srt_subtitles(script_data, audio_paths, output_dir)
            print(f"âœ… SRTå­—å¹•å·²ä¿å­˜: {srt_path}")
        except Exception as e:
            print(f"âš ï¸ SRTå­—å¹•å¯¼å‡ºå¤±è´¥: {e}")  # éå…³é”®åŠŸèƒ½ï¼Œå¤±è´¥ä¸ä¸­æ–­æµç¨‹

        return audio_paths

    except Exception as e:
        raise ValueError(f"è¯­éŸ³åˆæˆé”™è¯¯: {e}")


def export_srt_subtitles(script_data: Dict[str, Any], audio_paths: List[str], voice_dir: str) -> str:
    """
    å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶åˆ°voiceæ–‡ä»¶å¤¹
    
    Args:
        script_data: è„šæœ¬æ•°æ®
        audio_paths: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        voice_dir: voiceæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        str: SRTæ–‡ä»¶è·¯å¾„
    """
    from moviepy import AudioFileClip
    from core.video_composer import VideoComposer
    
    try:
        # è·å–å®é™…éŸ³é¢‘æ—¶é•¿
        segment_durations = []
        for audio_path in audio_paths:
            if os.path.exists(audio_path):
                clip = AudioFileClip(audio_path)
                segment_durations.append(float(clip.duration))
                clip.close()
            else:
                segment_durations.append(0.0)
        
        # å¤ç”¨VideoComposerçš„å­—å¹•åˆ†å‰²é€»è¾‘
        composer = VideoComposer()
        subtitle_config = config.SUBTITLE_CONFIG.copy()
        
        # ç”ŸæˆSRTå†…å®¹
        srt_lines = []
        subtitle_index = 1
        current_time = 0.0
        
        for i, segment in enumerate(script_data["segments"]):
            content = segment["content"]
            duration = segment_durations[i] if i < len(segment_durations) else 0.0
            
            # åˆ†å‰²æ–‡æœ¬
            subtitle_texts = composer.split_text_for_subtitle(
                content,
                subtitle_config["max_chars_per_line"],
                subtitle_config["max_lines"]
            )
            
            # è®¡ç®—æ¯è¡Œæ—¶é•¿
            if len(subtitle_texts) == 0:
                continue
                
            line_durations = []
            if len(subtitle_texts) == 1:
                line_durations = [duration]
            else:
                lengths = [max(1.0, len(t)) for t in subtitle_texts]
                total_len = sum(lengths)
                acc = 0.0
                for idx, length in enumerate(lengths):
                    if idx < len(lengths) - 1:
                        d = duration * (length / total_len)
                        line_durations.append(d)
                        acc += d
                    else:
                        line_durations.append(max(0.0, duration - acc))
            
            # ç”ŸæˆSRTæ¡ç›®
            for subtitle_text, subtitle_duration in zip(subtitle_texts, line_durations):
                start_time = current_time
                end_time = current_time + subtitle_duration
                
                # SRTæ—¶é—´æ ¼å¼
                start_srt = _format_srt_time(start_time)
                end_srt = _format_srt_time(end_time)
                
                srt_lines.append(f"{subtitle_index}")
                srt_lines.append(f"{start_srt} --> {end_srt}")
                srt_lines.append(subtitle_text.strip())
                srt_lines.append("")
                
                subtitle_index += 1
                current_time = end_time
        
        # å†™å…¥SRTæ–‡ä»¶
        project_name = os.path.basename(voice_dir.rstrip('/').rstrip('\\'))
        if project_name == "voice":
            project_name = os.path.basename(os.path.dirname(voice_dir.rstrip('/').rstrip('\\')))
        
        srt_filename = f"{project_name}_subtitles.srt"
        srt_path = os.path.join(voice_dir, srt_filename)
        
        with open(srt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(srt_lines))
        
        return srt_path
        
    except Exception as e:
        raise ValueError(f"SRTå­—å¹•å¯¼å‡ºé”™è¯¯: {e}")


def _format_srt_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºSRTæ ¼å¼ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

__all__ = [
    'generate_opening_image',
    'generate_images_for_segments',
    'generate_cover_images',
    'synthesize_voice_for_segments',
    'export_srt_subtitles'
]
