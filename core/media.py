"""
Media-related logic: opening image, images per segment, and TTS synthesis.
"""

from typing import Optional, Dict, Any, List, Tuple, Iterable, Set
import os
import concurrent.futures
import base64

from config import config
from prompts import (
    OPENING_IMAGE_STYLES,
    COVER_IMAGE_STYLE_PRESETS,
    COVER_IMAGE_PROMPT_TEMPLATE,
    IMAGE_STYLE_PRESETS,
    IMAGE_DESCRIPTION_PROMPT_TEMPLATE,
    IMAGE_PROMPT_SAFETY_TEMPLATE,
)
from core.utils import logger, ensure_directory_exists, APIError
from core.services import (
    text_to_image_doubao,
    text_to_audio_bytedance,
    text_to_image_siliconflow,
    text_to_text,
)
from core.validators import auto_detect_server_from_model

import requests


def _download_to_path(url: str, output_path: str, error_msg: str = "ä¸‹è½½å¤±è´¥") -> None:
    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
        with open(output_path, 'wb') as f:
            f.write(resp.content)
    except Exception as e:
        raise ValueError(f"{error_msg}: {e}")


def _persist_image_result(image_result: Dict[str, str], output_path: str, error_msg: str) -> None:
    """æ ¹æ®è¿”å›ç±»å‹å†™å…¥å›¾åƒæ–‡ä»¶ï¼Œæ”¯æŒURLæˆ–base64"""
    ensure_directory_exists(os.path.dirname(output_path))

    if not image_result:
        raise ValueError(f"{error_msg}: ç©ºå“åº”")

    data_type = image_result.get("type")
    data_value = image_result.get("data")

    if not data_type or not data_value:
        raise ValueError(f"{error_msg}: å“åº”ç¼ºå°‘å¿…è¦å­—æ®µ")

    try:
        if data_type == "url":
            _download_to_path(data_value, output_path, error_msg=error_msg)
        elif data_type == "b64":
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(data_value))
        else:
            raise ValueError(f"æœªçŸ¥çš„å›¾åƒæ•°æ®ç±»å‹: {data_type}")
    except Exception as e:
        raise ValueError(f"{error_msg}: {e}")


def _strip_code_fences(text: str) -> str:
    cleaned = (text or "").strip()
    if cleaned.startswith("```"):
        cleaned = cleaned.split("\n", 1)[1] if "\n" in cleaned else ""
    if cleaned.endswith("```"):
        cleaned = cleaned.rsplit("```", 1)[0]
    return cleaned.strip().strip('"').strip()


def _desensitize_image_prompt(original_prompt: str, safety_options: Optional[Dict[str, Any]]) -> Optional[str]:
    """ä½¿ç”¨LLMå¯¹å›¾åƒæç¤ºè¯è¿›è¡Œè„±æ•ï¼Œå¿…è¦æ—¶å›é€€ä¸»æ¨¡å‹ã€‚"""
    if not original_prompt or not safety_options:
        return None

    safety_model = safety_options.get("safety_model")
    primary_model = safety_options.get("llm_model")
    primary_server = safety_options.get("llm_server")
    max_tokens = safety_options.get("max_tokens", 800)
    temperature = safety_options.get("temperature", 0.2)

    candidates: List[Tuple[str, str]] = []
    if safety_model:
        safety_server = safety_options.get("safety_server") or auto_detect_server_from_model(safety_model, "llm")
        if safety_server:
            candidates.append((safety_server, safety_model))

    if primary_model:
        server = primary_server or auto_detect_server_from_model(primary_model, "llm")
        if server:
            candidates.append((server, primary_model))

    seen: Set[Tuple[str, str]] = set()
    for server, model in candidates:
        if not server or not model:
            continue
        key = (server, model)
        if key in seen:
            continue
        seen.add(key)

        try:
            response = text_to_text(
                server=server,
                model=model,
                prompt=IMAGE_PROMPT_SAFETY_TEMPLATE.format(prompt=original_prompt),
                system_message="",
                max_tokens=max_tokens,
                temperature=temperature,
            )
            cleaned = _strip_code_fences(response)
            if cleaned:
                logger.info(f"æç¤ºè¯è„±æ•æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹ {model}")
                return cleaned
        except APIError as api_error:
            message = str(api_error)
            if "æœªé…ç½®" in message or "API_KEY" in message.upper():
                logger.warning(f"æç¤ºè¯è„±æ•æ¨¡å‹ {model} ç¼ºå°‘å¯ç”¨çš„ API Keyï¼Œå°è¯•å›é€€æ¨¡å‹")
                continue
            logger.warning(f"æç¤ºè¯è„±æ•è°ƒç”¨å¤±è´¥: {api_error}")
            return None
        except Exception as exc:
            logger.warning(f"æç¤ºè¯è„±æ•å‡ºç°å¼‚å¸¸: {exc}")
            return None

    return None


def generate_opening_image(image_server: str, model: str, opening_style: str,
                           image_size: str, output_dir: str, opening_quote: bool = True) -> Optional[str]:
    """ç”Ÿæˆå¼€åœºå›¾åƒï¼Œå…¼å®¹å¤šç§æœåŠ¡å•†"""
    if not opening_quote:
        return None
    try:
        prompt = OPENING_IMAGE_STYLES.get(opening_style)
        if not prompt:
            default_style = next(iter(OPENING_IMAGE_STYLES))
            logger.warning(f"æœªæ‰¾åˆ°å¼€åœºå›¾åƒé£æ ¼: {opening_style}ï¼Œä½¿ç”¨é»˜è®¤é£æ ¼: {default_style}")
            prompt = OPENING_IMAGE_STYLES[default_style]
        prompt = str(prompt).strip()

        image_path = os.path.join(output_dir, "opening.png")

        if image_server == "siliconflow":
            image_result = text_to_image_siliconflow(
                prompt=prompt,
                size=image_size,
                model=model
            )
            _persist_image_result(image_result, image_path, "å¼€åœºå›¾åƒä¿å­˜å¤±è´¥")
        else:
            image_url = text_to_image_doubao(
                prompt=prompt,
                size=image_size,
                model=model
            )

            if not image_url:
                raise ValueError("å¼€åœºå›¾åƒç”Ÿæˆå¤±è´¥")

            _persist_image_result({"type": "url", "data": image_url}, image_path, "å¼€åœºå›¾åƒä¸‹è½½å¤±è´¥")

        logger.info(f"å¼€åœºå›¾åƒå·²ä¿å­˜: {image_path} (é£æ ¼: {opening_style})")
        print(f"å¼€åœºå›¾åƒå·²ä¿å­˜: {image_path}")
        return image_path
    except Exception as e:
        logger.warning(f"å¼€åœºå›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        return None


def _ensure_cover_style(style_id: str) -> Tuple[str, str]:
    """è·å–å°é¢é£æ ¼æè¿°ï¼Œè¿”å›(é£æ ¼id, style_text)ã€‚"""
    if style_id in COVER_IMAGE_STYLE_PRESETS:
        return style_id, COVER_IMAGE_STYLE_PRESETS[style_id]
    default_key = next(iter(COVER_IMAGE_STYLE_PRESETS))
    return default_key, COVER_IMAGE_STYLE_PRESETS[default_key]


def generate_cover_images(
    project_output_dir: str,
    image_server: str,
    model: str,
    image_size: str,
    style_id: str,
    count: int,
    video_title: str,
    content_title: str,
    cover_subtitle: str,
) -> Dict[str, Any]:
    """ç”Ÿæˆå°é¢å›¾åƒï¼Œä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œæ–‡ä»¶å cover_XX.pngã€‚"""
    try:
        if count < 1:
            count = 1

        os.makedirs(project_output_dir, exist_ok=True)
        style_key, style_text = _ensure_cover_style(style_id)
        prompt = COVER_IMAGE_PROMPT_TEMPLATE.format(
            video_title=video_title,
            content_title=content_title,
            cover_subtitle=cover_subtitle or video_title,
            style_block=style_text,
        )

        generated_paths: List[str] = []
        failures: List[str] = []

        for idx in range(1, count + 1):
            result = _generate_single_cover(
                image_server,
                model,
                image_size,
                prompt,
                project_output_dir,
                idx,
            )
            if result.get("success"):
                generated_paths.append(result["image_path"])
            else:
                failures.append(result.get("error", f"å°é¢{idx}ç”Ÿæˆå¤±è´¥"))

        return {
            "success": len(generated_paths) > 0,
            "cover_paths": generated_paths,
            "failures": failures,
            "style_id": style_key,
        }
    except Exception as e:
        raise ValueError(f"å°é¢å›¾åƒç”Ÿæˆé”™è¯¯: {e}")


def _generate_single_cover(
    image_server: str,
    model: str,
    image_size: str,
    prompt: str,
    project_output_dir: str,
    index: int,
) -> Dict[str, Any]:
    filename = f"cover_{index:02d}.png"
    image_path = os.path.join(project_output_dir, filename)

    try:
        if image_server == "siliconflow":
            image_result = text_to_image_siliconflow(
                prompt=prompt,
                size=image_size,
                model=model,
            )
            _persist_image_result(image_result, image_path, f"ä¿å­˜å°é¢å›¾åƒ {filename} å¤±è´¥")
        else:
            image_url = text_to_image_doubao(
                prompt=prompt,
                size=image_size,
                model=model,
            )
            if not image_url:
                raise ValueError("å°é¢å›¾åƒç”Ÿæˆè¿”å›ç©ºURL")
            _persist_image_result({"type": "url", "data": image_url}, image_path, f"ä¸‹è½½å°é¢å›¾åƒ {filename} å¤±è´¥")

        logger.info(f"å°é¢å›¾åƒå·²ä¿å­˜: {image_path}")
        print(f"å°é¢å›¾åƒå·²ä¿å­˜: {image_path}")
        return {"success": True, "image_path": image_path}
    except Exception as e:
        logger.warning(f"å°é¢å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        print(f"å°é¢å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def _generate_single_image(args) -> Dict[str, Any]:
    """ç”Ÿæˆå•ä¸ªå›¾åƒçš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰"""
    (
        segment_index,
        initial_prompt,
        model,
        image_size,
        output_dir,
        image_server,
        safety_options,
    ) = args

    print(f"æ­£åœ¨ç”Ÿæˆç¬¬{segment_index}æ®µå›¾åƒ...")
    logger.debug(f"ç¬¬{segment_index}æ®µå›¾åƒåˆå§‹æç¤ºè¯: {initial_prompt}")

    prompt_in_use = initial_prompt
    desensitize_attempts = 0
    max_desensitize = safety_options.get("max_attempts", MAX_PROMPT_SAFETY_ATTEMPTS) if safety_options else 0

    attempt = 0
    max_attempts = 3
    while attempt < max_attempts:
        try:
            image_path = os.path.join(output_dir, f"segment_{segment_index}.png")

            if image_server == "siliconflow":
                image_result = text_to_image_siliconflow(
                    prompt=prompt_in_use,
                    size=image_size,
                    model=model
                )
                _persist_image_result(image_result, image_path, f"ä¿å­˜ç¬¬{segment_index}æ®µå›¾åƒå¤±è´¥")
            else:
                image_url = text_to_image_doubao(
                    prompt=prompt_in_use,
                    size=image_size,
                    model=model
                )
                if not image_url:
                    raise ValueError("å›¾åƒç”Ÿæˆè¿”å›ç©ºURL")
                _persist_image_result({"type": "url", "data": image_url}, image_path, f"ä¸‹è½½ç¬¬{segment_index}æ®µå›¾åƒå¤±è´¥")

            print(f"ç¬¬{segment_index}æ®µå›¾åƒå·²ä¿å­˜: {image_path}")
            logger.info(f"ç¬¬{segment_index}æ®µå›¾åƒç”ŸæˆæˆåŠŸ: {image_path}")
            return {"success": True, "segment_index": segment_index, "image_path": image_path}

        except Exception as exc:  # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ŒåŒ…å« APIError
            error_msg = str(exc)
            lower_error = error_msg.lower()
            is_sensitive_error = (
                "outputimagesensitivecontentdetected" in lower_error
                or "sensitive" in lower_error
                or "content policy" in lower_error
            )

            if is_sensitive_error and safety_options and desensitize_attempts < max_desensitize:
                desensitize_attempts += 1
                print(f"ç¬¬{segment_index}æ®µæç¤ºè¯è§¦å‘æ•æ„Ÿå®¡æ ¸ï¼Œå°è¯•è‡ªåŠ¨è„±æ•ï¼ˆç¬¬{desensitize_attempts}/{max_desensitize}æ¬¡ï¼‰")
                logger.warning(f"ç¬¬{segment_index}æ®µæç¤ºè¯æ¶‰åŠæ•æ„Ÿå†…å®¹ï¼Œå°è¯•è‡ªåŠ¨è„±æ•ï¼š{error_msg}")
                sanitized = _desensitize_image_prompt(prompt_in_use, safety_options)
                if sanitized:
                    prompt_in_use = sanitized
                    logger.debug(f"ç¬¬{segment_index}æ®µå›¾åƒæç¤ºè¯å·²è„±æ•: {prompt_in_use}")
                    continue
                else:
                    logger.warning(f"ç¬¬{segment_index}æ®µæç¤ºè¯è„±æ•å¤±è´¥")

            attempt += 1

            if attempt < max_attempts:
                logger.warning(
                    f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼š{error_msg}ï¼Œå‡†å¤‡é‡è¯•ï¼ˆç¬¬{attempt + 1}/{max_attempts}æ¬¡ï¼‰"
                )
                continue

            logger.warning(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯ï¼š{error_msg}")
            print(f"ç¬¬{segment_index}æ®µå›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡")
            break

    return {"success": False, "segment_index": segment_index, "image_path": ""}


def generate_images_for_segments(
    image_server: str,
    model: str,
    script_data: Dict[str, Any],
    image_style_preset: str,
    image_size: str,
    output_dir: str,
    images_method: str = "keywords",
    keywords_data: Optional[Dict[str, Any]] = None,
    description_data: Optional[Dict[str, Any]] = None,
    target_segments: Optional[Iterable[int]] = None,
    llm_model: Optional[str] = None,
    llm_server: Optional[str] = None,
) -> Dict[str, Any]:
    """ä¸ºæ¯ä¸ªæ®µè½ç”Ÿæˆå›¾åƒï¼ˆæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ï¼‰"""
    try:
        try:
            image_style = IMAGE_STYLE_PRESETS.get(
                image_style_preset,
                next(iter(IMAGE_STYLE_PRESETS.values()))
            )
        except Exception:
            image_style = ""
        logger.info(
            f"ä½¿ç”¨å›¾åƒæœåŠ¡: {image_server}ï¼Œé£æ ¼: {image_style_preset} -> {image_style}ï¼Œæ¨¡å¼: {images_method}"
        )

        images_method = images_method or getattr(config, "SUPPORTED_IMAGE_METHODS", ["keywords"])[0]
        segments = script_data.get("segments", [])
        if not segments:
            raise ValueError("è„šæœ¬æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå›¾åƒ")

        segment_count = len(segments)
        has_specific_selection = target_segments is not None
        target_set: Set[int] = set()
        if has_specific_selection:
            normalized_targets = list(target_segments or [])
            parsed_targets: Set[int] = set()
            for idx in normalized_targets:
                try:
                    parsed_targets.add(int(idx))
                except (TypeError, ValueError):
                    continue
            target_set = {idx for idx in parsed_targets if 1 <= idx <= segment_count}
            if not target_set and parsed_targets:
                raise ValueError("æœªæ‰¾åˆ°éœ€è¦ç”Ÿæˆå›¾åƒçš„æœ‰æ•ˆæ®µè½")

        prompt_payload: List[tuple[int, str]] = []

        if images_method == "description":
            summary_text = (description_data or {}).get("summary", "").strip()
            if not summary_text:
                raise ValueError("ç¼ºå°‘æè¿°æ¨¡å¼æ‰€éœ€çš„å°ç»“å†…å®¹")
            template = IMAGE_DESCRIPTION_PROMPT_TEMPLATE
            default_style = getattr(
                config,
                "DESCRIPTION_DEFAULT_STYLE_GUIDANCE",
                "ç”»é¢éœ€ä¿æŒä¿¡æ¯æ¸…æ™°ã€æ„å›¾ç¨³å®šã€è‰²å½©å’Œè°ã€‚"
            )
            for segment in segments:
                segment_index = int(segment.get("index") or len(prompt_payload) + 1)
                if target_set:
                    if segment_index not in target_set:
                        continue
                elif has_specific_selection:
                    continue
                segment_content = segment.get("content", "")
                style_block = image_style or default_style
                final_prompt = template.format(
                    summary=summary_text,
                    segment=segment_content,
                    style_block=style_block
                )
                prompt_payload.append((segment_index, final_prompt))
        else:
            if not keywords_data:
                raise ValueError("ç¼ºå°‘å…³é”®è¯æ•°æ®")
            keyword_segments = list(keywords_data.get("segments", []))
            if len(keyword_segments) < len(segments):
                keyword_segments.extend(
                    [{"keywords": [], "atmosphere": []}] * (len(segments) - len(keyword_segments))
                )
            for idx, segment in enumerate(segments, 1):
                segment_keywords = keyword_segments[idx - 1] if idx - 1 < len(keyword_segments) else {}
                keywords = segment_keywords.get("keywords", [])
                atmosphere = segment_keywords.get("atmosphere", [])
                style_part = f"[é£æ ¼] {image_style}" if image_style else ""
                content_parts: List[str] = []
                content_parts.extend(keywords)
                content_parts.extend(atmosphere)
                content_part = f"[å†…å®¹] {' | '.join(content_parts)}" if content_parts else ""
                sections = [part for part in [style_part, content_part] if part]
                final_prompt = "\n".join(sections) if sections else image_style
                if not final_prompt:
                    final_prompt = f"[å†…å®¹] {segment.get('content', '')}".strip()
                segment_index = segment.get('index') or idx
                if target_set:
                    if segment_index not in target_set:
                        continue
                elif has_specific_selection:
                    continue
                prompt_payload.append((segment_index, final_prompt))

        if not prompt_payload:
            raise ValueError("æœªç”Ÿæˆæœ‰æ•ˆçš„æç¤ºè¯")

        max_workers = getattr(config, "MAX_CONCURRENT_IMAGE_GENERATION", 3)
        print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹ç”Ÿæˆå›¾åƒ... (æœ¬æ¬¡å¤„ç† {len(prompt_payload)} æ®µ)")

        image_paths: List[str] = [""] * segment_count
        for idx, segment in enumerate(segments, 1):
            segment_index = int(segment.get("index") or idx)
            position = segment_index - 1
            if 0 <= position < segment_count:
                existing_path = os.path.join(output_dir, f"segment_{segment_index}.png")
                if os.path.exists(existing_path):
                    image_paths[position] = existing_path
        failed_segments: List[int] = []

        safety_options: Optional[Dict[str, Any]] = None
        if llm_model:
            safety_options = {
                "llm_model": llm_model,
                "llm_server": llm_server,
                "max_attempts": MAX_PROMPT_SAFETY_ATTEMPTS,
                "max_tokens": 800,
                "temperature": 0.2,
            }

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_index = {
                executor.submit(
                    _generate_single_image,
                    (int(idx), prompt, model, image_size, output_dir, image_server, safety_options or {})
                ): int(idx)
                for idx, prompt in prompt_payload
            }

            for future in concurrent.futures.as_completed(future_to_index):
                result = future.result()
                segment_index = int(result["segment_index"])
                position = segment_index - 1
                if result["success"] and 0 <= position < segment_count:
                    image_paths[position] = result["image_path"]
                else:
                    failed_segments.append(segment_index)
                    if 0 <= position < segment_count:
                        image_paths[position] = ""

        return {
            "image_paths": image_paths,
            "failed_segments": failed_segments,
            "processed_segments": sorted(target_set) if target_set else list(range(1, segment_count + 1)),
        }

    except Exception as e:
        raise ValueError(f"å›¾åƒç”Ÿæˆé”™è¯¯: {e}")


def _resolve_existing_voice_path(output_dir: str, segment_index: int) -> Optional[str]:
    """æŸ¥æ‰¾å·²å­˜åœ¨çš„æ®µè½è¯­éŸ³æ–‡ä»¶ï¼Œä¼˜å…ˆè¿”å›wavç‰ˆæœ¬"""
    candidates = [
        os.path.join(output_dir, f"voice_{segment_index}.wav"),
        os.path.join(output_dir, f"voice_{segment_index}.mp3"),
    ]
    for path in candidates:
        if os.path.exists(path):
            return path
    return None


def _synthesize_single_voice(args) -> Dict[str, Any]:
    """
    åˆæˆå•ä¸ªè¯­éŸ³çš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
    """
    segment_index, content, server, voice, output_dir, speed_ratio, loudness_ratio = args
    
    print(f"æ­£åœ¨ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³...")
    
    audio_filename = f"voice_{segment_index}.wav"
    audio_path = os.path.join(output_dir, audio_filename)
    
    try:
        if server == "bytedance":
            success = text_to_audio_bytedance(
                text=content,
                output_filename=audio_path,
                voice=voice,
                speed_ratio=speed_ratio,
                loudness_ratio=loudness_ratio,
            )
        else:
            return {"success": False, "segment_index": segment_index, "error": f"ä¸æ”¯æŒçš„TTSæœåŠ¡å•†: {server}"}
        
        if success:
            print(f"ç¬¬{segment_index}æ®µè¯­éŸ³å·²ä¿å­˜: {audio_path}")
            return {"success": True, "segment_index": segment_index, "audio_path": audio_path}
        else:
            return {"success": False, "segment_index": segment_index, "error": f"ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³å¤±è´¥"}
    
    except Exception as e:
        return {"success": False, "segment_index": segment_index, "error": str(e)}


def synthesize_voice_for_segments(
    server: str,
    voice: str,
    script_data: Dict[str, Any],
    output_dir: str,
    target_segments: Optional[Iterable[int]] = None,
    speed_ratio: float = 1.0,
    loudness_ratio: float = 1.0,
) -> List[str]:
    """
    ä¸ºæ¯ä¸ªæ®µè½åˆæˆè¯­éŸ³ï¼ˆæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ï¼‰
    """
    try:
        segments = script_data.get("segments", [])
        if not segments:
            raise ValueError("è„šæœ¬æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¯­éŸ³")

        segment_count = len(segments)
        has_specific_selection = target_segments is not None
        target_set: Set[int] = set()
        if has_specific_selection:
            normalized_targets = list(target_segments or [])
            parsed_targets: Set[int] = set()
            for idx in normalized_targets:
                try:
                    parsed_targets.add(int(idx))
                except (TypeError, ValueError):
                    continue
            target_set = {idx for idx in parsed_targets if 1 <= idx <= segment_count}
            if not target_set and parsed_targets:
                raise ValueError("æœªæ‰¾åˆ°éœ€è¦ç”Ÿæˆè¯­éŸ³çš„æœ‰æ•ˆæ®µè½")

        audio_paths: List[str] = [""] * segment_count
        task_args: List[Tuple[int, str, str, str, str, float, float]] = []

        for idx, segment in enumerate(segments, 1):
            segment_index = int(segment.get("index") or idx)
            position = segment_index - 1
            if not (0 <= position < segment_count):
                continue

            if has_specific_selection:
                if target_set:
                    if segment_index not in target_set:
                        existing = _resolve_existing_voice_path(output_dir, segment_index)
                        if existing:
                            audio_paths[position] = existing
                        continue
                else:
                    existing = _resolve_existing_voice_path(output_dir, segment_index)
                    if existing:
                        audio_paths[position] = existing
                    continue

            content = segment.get("content", "")
            task_args.append((segment_index, content, server, voice, output_dir, speed_ratio, loudness_ratio))

        if task_args:
            max_workers = getattr(config, "MAX_CONCURRENT_VOICE_SYNTHESIS", 2)
            print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹åˆæˆè¯­éŸ³... (æœ¬æ¬¡å¤„ç† {len(task_args)} æ®µ)")

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_index = {
                    executor.submit(_synthesize_single_voice, args): args[0]
                    for args in task_args
                }

                for future in concurrent.futures.as_completed(future_to_index):
                    result = future.result()
                    segment_index = result["segment_index"]
                    if result["success"]:
                        position = segment_index - 1
                        if 0 <= position < segment_count:
                            audio_paths[position] = result["audio_path"]
                    else:
                        error_msg = result.get("error", f"ç”Ÿæˆç¬¬{segment_index}æ®µè¯­éŸ³å¤±è´¥")
                        raise ValueError(error_msg)
        else:
            print("æœ¬æ¬¡æœªé€‰æ‹©ä»»ä½•æ®µè½ç”Ÿæˆè¯­éŸ³ï¼Œä¿æŒç°æœ‰éŸ³é¢‘ã€‚")

        # è¡¥å…¨æœªå¤„ç†æ®µè½çš„éŸ³é¢‘è·¯å¾„å¹¶è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒ
        for idx in range(1, segment_count + 1):
            position = idx - 1
            if audio_paths[position]:
                continue
            existing = _resolve_existing_voice_path(output_dir, idx)
            if existing:
                audio_paths[position] = existing
            else:
                raise ValueError(f"ç¬¬{idx}æ®µè¯­éŸ³ç¼ºå¤±ï¼Œè¯·å…ˆå®Œæˆæ•´ä½“åˆæˆ")

        # è¯­éŸ³åˆæˆå®Œæˆåï¼Œç«‹å³å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶
        print("ğŸ¬ å¼€å§‹å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶...")
        try:
            srt_path = export_srt_subtitles(script_data, audio_paths, output_dir)
            print(f"âœ… SRTå­—å¹•å·²ä¿å­˜: {srt_path}")
        except Exception as e:
            print(f"âš ï¸ SRTå­—å¹•å¯¼å‡ºå¤±è´¥: {e}")  # éå…³é”®åŠŸèƒ½ï¼Œå¤±è´¥ä¸ä¸­æ–­æµç¨‹

        return audio_paths

    except Exception as e:
        raise ValueError(f"è¯­éŸ³åˆæˆé”™è¯¯: {e}")


def export_srt_subtitles(script_data: Dict[str, Any], audio_paths: List[str], voice_dir: str) -> str:
    """
    å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶åˆ°voiceæ–‡ä»¶å¤¹
    
    Args:
        script_data: è„šæœ¬æ•°æ®
        audio_paths: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        voice_dir: voiceæ–‡ä»¶å¤¹è·¯å¾„
    
    Returns:
        str: SRTæ–‡ä»¶è·¯å¾„
    """
    from moviepy import AudioFileClip
    from core.video_composer import VideoComposer
    
    try:
        # è·å–å®é™…éŸ³é¢‘æ—¶é•¿
        segment_durations = []
        for audio_path in audio_paths:
            if os.path.exists(audio_path):
                clip = AudioFileClip(audio_path)
                segment_durations.append(float(clip.duration))
                clip.close()
            else:
                segment_durations.append(0.0)
        
        # å¤ç”¨VideoComposerçš„å­—å¹•åˆ†å‰²é€»è¾‘
        composer = VideoComposer()
        subtitle_config = config.SUBTITLE_CONFIG.copy()
        
        # ç”ŸæˆSRTå†…å®¹
        srt_lines = []
        subtitle_index = 1
        current_time = 0.0
        
        for i, segment in enumerate(script_data["segments"]):
            content = segment["content"]
            duration = segment_durations[i] if i < len(segment_durations) else 0.0
            
            # åˆ†å‰²æ–‡æœ¬
            subtitle_texts = composer.split_text_for_subtitle(
                content,
                subtitle_config["max_chars_per_line"],
                subtitle_config["max_lines"]
            )
            
            # è®¡ç®—æ¯è¡Œæ—¶é•¿
            if len(subtitle_texts) == 0:
                continue
                
            line_durations = []
            if len(subtitle_texts) == 1:
                line_durations = [duration]
            else:
                lengths = [max(1.0, len(t)) for t in subtitle_texts]
                total_len = sum(lengths)
                acc = 0.0
                for idx, length in enumerate(lengths):
                    if idx < len(lengths) - 1:
                        d = duration * (length / total_len)
                        line_durations.append(d)
                        acc += d
                    else:
                        line_durations.append(max(0.0, duration - acc))
            
            # ç”ŸæˆSRTæ¡ç›®
            for subtitle_text, subtitle_duration in zip(subtitle_texts, line_durations):
                start_time = current_time
                end_time = current_time + subtitle_duration
                
                # SRTæ—¶é—´æ ¼å¼
                start_srt = _format_srt_time(start_time)
                end_srt = _format_srt_time(end_time)
                
                srt_lines.append(f"{subtitle_index}")
                srt_lines.append(f"{start_srt} --> {end_srt}")
                srt_lines.append(subtitle_text.strip())
                srt_lines.append("")
                
                subtitle_index += 1
                current_time = end_time
        
        # å†™å…¥SRTæ–‡ä»¶
        project_name = os.path.basename(voice_dir.rstrip('/').rstrip('\\'))
        if project_name == "voice":
            project_name = os.path.basename(os.path.dirname(voice_dir.rstrip('/').rstrip('\\')))
        
        srt_filename = f"{project_name}_subtitles.srt"
        srt_path = os.path.join(voice_dir, srt_filename)
        
        with open(srt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(srt_lines))
        
        return srt_path
        
    except Exception as e:
        raise ValueError(f"SRTå­—å¹•å¯¼å‡ºé”™è¯¯: {e}")


def _format_srt_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºSRTæ ¼å¼ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

MAX_PROMPT_SAFETY_ATTEMPTS = 3


__all__ = [
    'generate_opening_image',
    'generate_images_for_segments',
    'generate_cover_images',
    'synthesize_voice_for_segments',
    'export_srt_subtitles'
]
