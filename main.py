import os
import json
import datetime
import glob
from functions import (
    read_document, intelligent_summarize, extract_keywords, 
    generate_images_for_segments, synthesize_voice_for_segments, 
    compose_final_video
)
from config import config


def auto_detect_server_from_model(model_name: str, model_type: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ£€æµ‹æœåŠ¡å•†
    
    Args:
        model_name: æ¨¡å‹åç§°
        model_type: æ¨¡å‹ç±»å‹ (llm/image/tts)
    
    Returns:
        str: æœåŠ¡å•†åç§°
    """
    if model_type == "llm":
        # LLMæ¨¡å‹æœåŠ¡å•†è¯†åˆ«
        if any(prefix in model_name for prefix in ["google/", "anthropic/", "meta/"]):
            return "openrouter"
        elif any(prefix in model_name for prefix in ["zai-org/", "moonshotai/", "Qwen/"]):
            return "siliconflow"
        elif model_name.startswith("gpt-"):
            return "openai"  # aihubmixä»£ç†
        else:
            return "openrouter"  # é»˜è®¤
    
    elif model_type == "image":
        # å›¾åƒæ¨¡å‹æœåŠ¡å•†è¯†åˆ«
        if "doubao" in model_name.lower() or "seedream" in model_name.lower():
            return "doubao"
        else:
            return "doubao"  # é»˜è®¤
    
    elif model_type == "voice":
        # è¯­éŸ³æœåŠ¡å•†è¯†åˆ«
        if "_bigtts" in model_name:
            return "bytedance"  # å­—èŠ‚è¯­éŸ³åˆæˆå¤§æ¨¡å‹
        else:
            return "bytedance"  # å½“å‰åªæ”¯æŒå­—èŠ‚è¯­éŸ³åˆæˆ
    
    return "unknown"

def main(
    input_file=None,    # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆEPUBæˆ–PDFæ–‡ä»¶ï¼‰ï¼Œå¦‚ä¸ºNoneåˆ™è‡ªåŠ¨ä»inputæ–‡ä»¶å¤¹è¯»å–
    target_length=800,  # ç¼©å†™åçš„ç›®æ ‡å­—æ•°ï¼ŒèŒƒå›´500-1000å­—
    num_segments=10,    # åˆ†æ®µæ•°é‡ï¼Œé»˜è®¤10æ®µ
    image_size="1280x720",  # ç”Ÿæˆå›¾ç‰‡çš„å°ºå¯¸ï¼Œå¯é€‰ï¼š1024x1024(1:1), 1280x720(16:9), 864x1152(3:4), 720x1280(9:16)ç­‰
    llm_model="google/gemini-2.5-pro",  # å¤§è¯­è¨€æ¨¡å‹
    image_model="doubao-seedream-3-0-t2i-250415",  # å›¾åƒç”Ÿæˆæ¨¡å‹
    voice="zh_male_yuanboxiaoshu_moon_bigtts",      # è¯­éŸ³éŸ³è‰²
    output_dir="output",  # è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„outputæ–‡ä»¶å¤¹
    image_style_preset="cinematic",  # å›¾åƒé£æ ¼é¢„è®¾ï¼Œå¯é€‰ï¼šcinematic, documentary, artisticç­‰
    enable_subtitles=True  # æ˜¯å¦å¯ç”¨å­—å¹•ï¼Œé»˜è®¤å¯ç”¨
):
    
    try:
        start_time = datetime.datetime.now()
        
        # è‡ªåŠ¨è¯†åˆ«æœåŠ¡å•†
        llm_server = auto_detect_server_from_model(llm_model, "llm")
        image_server = auto_detect_server_from_model(image_model, "image") 
        tts_server = auto_detect_server_from_model(voice, "voice")
        
        # Input validation
        if not 500 <= target_length <= 1000:
            raise ValueError("target_lengthå¿…é¡»åœ¨500-1000ä¹‹é—´")
        if not 5 <= num_segments <= 20:
            raise ValueError("num_segmentså¿…é¡»åœ¨5-20ä¹‹é—´")
        if llm_server not in ["openrouter", "openai", "siliconflow"]:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæ¨¡å‹: {llm_model}ï¼Œè¯·ä½¿ç”¨æ”¯æŒçš„æ¨¡å‹")
        if image_server not in ["doubao"]:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒæ¨¡å‹: {image_model}ï¼Œè¯·ä½¿ç”¨æ”¯æŒçš„æ¨¡å‹")
        if tts_server not in ["bytedance"]:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­éŸ³æ¨¡å‹: {voice}ï¼Œè¯·ä½¿ç”¨æ”¯æŒçš„è¯­éŸ³")
        if image_size not in config.SUPPORTED_IMAGE_SIZES:
            print(f"\nâš ï¸  ä¸æ”¯æŒçš„å›¾åƒå°ºå¯¸: {image_size}")
            print("æ”¯æŒçš„å°ºå¯¸: " + ", ".join(config.SUPPORTED_IMAGE_SIZES))
            raise ValueError(f"è¯·é€‰æ‹©æ”¯æŒçš„å›¾åƒå°ºå¯¸")

        # 1. æ–‡æ¡£è¯»å–
        if input_file is None:
            # è‡ªåŠ¨ä»inputæ–‡ä»¶å¤¹è¯»å–æ–‡ä»¶
            input_files = glob.glob("input/*.epub") + glob.glob("input/*.pdf")
            if not input_files:
                raise ValueError("inputæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°EPUBæˆ–PDFæ–‡ä»¶")
            input_file = input_files[0]
        
        print(f"æ­£åœ¨è¯»å–æ–‡æ¡£: {input_file}")
        document_content, original_length = read_document(input_file)
        
        # 2. æ™ºèƒ½ç¼©å†™ï¼ˆç¬¬ä¸€æ¬¡LLMå¤„ç†ï¼‰
        print("æ­£åœ¨è¿›è¡Œæ™ºèƒ½ç¼©å†™å¤„ç†...")
        script_data = intelligent_summarize(
            llm_server, llm_model, document_content, 
            target_length, num_segments
        )
        
        # åˆ›å»ºå¸¦æœ‰title+æ—¶é—´çš„è¾“å‡ºç›®å½•ç»“æ„
        current_time = datetime.datetime.now()
        time_suffix = current_time.strftime("%m%d_%H%M")
        title = script_data.get('title', 'untitled').replace(' ', '_').replace('/', '_').replace('\\', '_')
        project_folder = f"{title}_{time_suffix}"
        project_output_dir = os.path.join(output_dir, project_folder)
        
        os.makedirs(project_output_dir, exist_ok=True)
        os.makedirs(f"{project_output_dir}/images", exist_ok=True)
        os.makedirs(f"{project_output_dir}/voice", exist_ok=True)
        os.makedirs(f"{project_output_dir}/text", exist_ok=True)
        
        print(f"é¡¹ç›®è¾“å‡ºç›®å½•: {project_output_dir}")
        
        # ä¿å­˜å£æ’­ç¨¿JSON
        script_path = f"{project_output_dir}/text/script.json"
        with open(script_path, 'w', encoding='utf-8') as f:
            json.dump(script_data, f, ensure_ascii=False, indent=2)
        print(f"å£æ’­ç¨¿å·²ä¿å­˜åˆ°: {script_path}")
        
        # 3. å…³é”®è¯æå–ï¼ˆç¬¬äºŒæ¬¡LLMå¤„ç†ï¼‰
        print("æ­£åœ¨æå–å…³é”®è¯...")
        keywords_data = extract_keywords(
            llm_server, llm_model, script_data
        )
        
        # ä¿å­˜å…³é”®è¯JSON
        keywords_path = f"{project_output_dir}/text/keywords.json"
        with open(keywords_path, 'w', encoding='utf-8') as f:
            json.dump(keywords_data, f, ensure_ascii=False, indent=2)
        print(f"å…³é”®è¯å·²ä¿å­˜åˆ°: {keywords_path}")
        
        # 4. AIå›¾åƒç”Ÿæˆ
        print("æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
        image_paths = generate_images_for_segments(
            image_server, image_model, keywords_data, 
            image_style_preset, image_size, f"{project_output_dir}/images"
        )
        
        # 5. è¯­éŸ³åˆæˆ
        print("æ­£åœ¨åˆæˆè¯­éŸ³...")
        audio_paths = synthesize_voice_for_segments(
            tts_server, voice, script_data, f"{project_output_dir}/voice"
        )
        
        # 6. è§†é¢‘åˆæˆ
        print("æ­£åœ¨åˆæˆæœ€ç»ˆè§†é¢‘...")
        final_video_path = compose_final_video(
            image_paths, audio_paths, f"{project_output_dir}/final_video.mp4",
            script_data=script_data, enable_subtitles=enable_subtitles
        )
        
        # ç”Ÿæˆå¤„ç†æ‘˜è¦
        end_time = datetime.datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        compression_ratio = (1 - script_data['total_length'] / original_length) * 100
        
        summary_text = f"""=== æ–‡æ¡£å¤„ç†æ‘˜è¦ ===
å¤„ç†æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}
åŸå§‹æ–‡æ¡£: {os.path.basename(input_file)}
åŸå§‹å­—æ•°: {original_length:,}å­—
ç›®æ ‡å­—æ•°: {target_length}å­—
å®é™…å­—æ•°: {script_data['total_length']}å­—
å‹ç¼©æ¯”ä¾‹: {compression_ratio:.1f}%
åˆ†æ®µæ•°é‡: {num_segments}æ®µ
å›¾åƒé£æ ¼: {image_style_preset}
å­—å¹•åŠŸèƒ½: {'å¯ç”¨' if enable_subtitles else 'ç¦ç”¨'}
æ€»å¤„ç†æ—¶é—´: {execution_time:.1f}ç§’
"""
        
        summary_path = f"{project_output_dir}/text/summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        print(f"å¤„ç†æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")
        
        # è¾“å‡ºå®Œæˆä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ‰ è§†é¢‘åˆ¶ä½œå®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“„ å£æ’­ç¨¿æ®µæ•°: {script_data['actual_segments']}")
        print(f"ğŸ–¼ï¸  ç”Ÿæˆå›¾ç‰‡æ•°é‡: {len(image_paths)}")
        print(f"ğŸ”Š éŸ³é¢‘æ–‡ä»¶æ•°é‡: {len(audio_paths)}")
        print(f"ğŸ¬ æœ€ç»ˆè§†é¢‘: {final_video_path}")
        print(f"ğŸ“ å­—å¹•åŠŸèƒ½: {'å¯ç”¨' if enable_subtitles else 'ç¦ç”¨'}")
        print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {execution_time:.1f}ç§’")
        print("="*60)
        
        # è¿”å›ç»“æœ
        result = {
            "success": True,
            "message": "è§†é¢‘åˆ¶ä½œå®Œæˆ",
            "execution_time": execution_time,
            "script": {
                "file_path": script_path,
                "total_length": script_data['total_length'],
                "segments_count": script_data['actual_segments']
            },
            "keywords": {
                "file_path": keywords_path,
                "total_keywords": sum(len(seg.get('keywords', [])) + len(seg.get('atmosphere', [])) 
                                    for seg in keywords_data['segments']),
                "avg_per_segment": sum(len(seg.get('keywords', [])) + len(seg.get('atmosphere', [])) 
                                     for seg in keywords_data['segments']) / len(keywords_data['segments'])
            },
            "text_files": {
                "summary": summary_path
            },
            "images": image_paths,
            "audio_files": audio_paths,
            "final_video": final_video_path,
            "statistics": {
                "original_length": original_length,
                "compression_ratio": f"{compression_ratio:.1f}%",
                "total_processing_time": execution_time,
                "llm_calls": 2,
                "image_generation_time": 0,  # Will be updated by actual implementation
                "audio_generation_time": 0,  # Will be updated by actual implementation
                "video_composition_time": 0  # Will be updated by actual implementation
            }
        }
        
        return result
    
    except Exception as e:
        return {
            "success": False,
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "execution_time": 0,
            "error": str(e)
        }

# Run the main function
if __name__ == "__main__":
    print("ğŸš€ æ™ºèƒ½è§†é¢‘åˆ¶ä½œç³»ç»Ÿå¯åŠ¨")
    
    # ========================================================================
    # å¯é€‰å‚æ•°è¯´æ˜ (æ‰€æœ‰æ¨¡å‹åç§°å‡å¯ç›´æ¥å¤åˆ¶ç²˜è´´ä½¿ç”¨)
    # ========================================================================
    
    # åŸºç¡€å‚æ•°
    # target_length: ç›®æ ‡å­—æ•° (500-1000)
    # num_segments: åˆ†æ®µæ•°é‡ (5-20) 
    # enable_subtitles: æ˜¯å¦å¯ç”¨å­—å¹• (True/False)
    
    # å›¾åƒå°ºå¯¸é€‰é¡¹
    # image_size: 1024x1024 | 1280x720 | 720x1280 | 864x1152 | 1152x864 | 832x1248 | 1248x832 | 1512x648
    
    # LLMæ¨¡å‹é€‰é¡¹
    # llm_model:
    #     OpenRouteræœåŠ¡å•†:
    #       - google/gemini-2.5-pro
    #       - anthropic/claude-sonnet-4  
    #       - anthropic/claude-3.7-sonnet:thinking
    #     
    #     SiliconFlowæœåŠ¡å•†:
    #       - zai-org/GLM-4.5
    #       - moonshotai/Kimi-K2-Instruct
    #       - Qwen/Qwen3-235B-A22B-Thinking-2507
    #     
    #     OpenAIæœåŠ¡å•†(aihubmixä»£ç†):
    #       - gpt-5
    
    # å›¾åƒç”Ÿæˆæ¨¡å‹
    # image_model: doubao-seedream-3-0-t2i-250415
    
    # è¯­éŸ³éŸ³è‰²é€‰é¡¹  
    # voice: zh_male_yuanboxiaoshu_moon_bigtts | zh_female_linjianvhai_moon_bigtts | 
    #        zh_male_yangguangqingnian_moon_bigtts | ICL_zh_female_heainainai_tob
    
    # å›¾åƒé£æ ¼é¢„è®¾
    # image_style_preset: cinematic | documentary | artistic | minimalist | vintage
    # ========================================================================
    
    # è¿è¡Œä¸»ç¨‹åº
    main(
        target_length=800,
        num_segments=10,
        image_size="1280x720",
        llm_model="moonshotai/Kimi-K2-Instruct",
        image_model="doubao-seedream-3-0-t2i-250415",
        voice="zh_male_yuanboxiaoshu_moon_bigtts",
        image_style_preset="cinematic",
        enable_subtitles=True
    )